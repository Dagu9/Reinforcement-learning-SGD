
###### General parameters ######

[GENERAL]
domains = Banks
singledomain = True
tracedialog = 0
seed = 30061523

[exec_config]
domain = Banks
configdir = _configs
logfiledir = _logs
numtrainbatches = 50
traindialogsperbatch = 100
numbatchtestdialogs =  300
trainsourceiteration = 0
numtestdialogs =  300
trainerrorrate = 0
testerrorrate  = 0
testeverybatch = True
supervised = True
numsuperviseddialogs = 70
trainingfile = data/train/Banks.md
actionslotsuccessreward = 10
actiononlysuccessreward = 5
actionwrongpenalty = 0

[logging]
usecolor = False
screen_level = results
file_level = dial
file = auto

###### Environment parameters ######

[agent]
maxturns = 15

[usermodel]
usenewgoalscenarios = True
oldstylepatience = False
patience = 15

[errormodel]
nbestsize = 1
nbestgeneratormodel = SampledNBestGenerator
confscorer = additive

[summaryacts]
maxinformslots = 5
informmask = True
requestmask = True
informcountaccepted = 4
byemask = True

###### User simulator ######
[usermodel_Banks]
usersimulator = usersimulator.myUserSim.SGD_user_simulator.SGDUserSimulator
goalgenerator = usersimulator.myUserSim.SGD_goal_generator.SGDGoalGenerator

###### Dialogue Manager parameters ######
[ontology_Banks]
handler = ontology.FlatOntologyManager.FlatDomainOntology

[dqnpolicy]
maxiter = 10000
gamma = 0.99
learning_rate = 0.001
tau = 0.02
replay_type = vanilla
minibatch_size = 128
capacity = 10000
exploration_type = e-greedy
episodeNum= 0.0
epsilon_start = 0.3
epsilon_end = 0.0
features = ["discourseAct", "method", "requested", "full", "lastActionInformNone", "offerHappened", "inform_info"]
max_k = 5
learning_algorithm = dqn
architecture = vanilla
h1_size = 300
h2_size = 200
training_frequency = 1
n_samples = 1
stddev_var_mu = 0.01
stddev_var_logsigma = 0.01
mean_log_sigma = 0.000001
sigma_prior = 1.5
alpha =0.85
alpha_divergence =False
sigma_eps = 0.01
delta = 1.0
beta = 0.95
is_threshold = 5.0
train_iters_per_episode = 1

[policy_Banks]
belieftype = belieftracking.myBeliefTracker.SGD_beliefTracker.SGDBeliefTracker
policytype = dqn
policydir = _policies 
useconfreq = False
learning = True
startwithhello = False
inpolicyfile = auto
outpolicyfile = auto

###### Evaluation parameters ######
[eval]
rewardvenuerecommended = 0
penaliseallturns = True
wrongvenuepenalty = 0
notmentionedvaluepenalty = 0
successmeasure = objective 
successreward = 15 
failpenalty = 0

